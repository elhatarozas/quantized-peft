{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c7b29670",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ell/playground/env/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, BitsAndBytesConfig, AutoTokenizer\n",
    "import re \n",
    "from peft import PeftModel, LoraConfig, TaskType, LoftQConfig, prepare_model_for_kbit_training\n",
    "from datasets import load_dataset\n",
    "from tqdm import tqdm\n",
    "import os "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88a0ddd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3b2002e",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.get_device_name(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ff508729",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ell/playground/env/lib/python3.10/site-packages/peft/tuners/lora/config.py:635: UserWarning: `loftq_config` specified but will be ignored when `init_lora_weights` is not 'loftq'.\n",
      "  warnings.warn(\"`loftq_config` specified but will be ignored when `init_lora_weights` is not 'loftq'.\")\n"
     ]
    }
   ],
   "source": [
    "MODEL_ID = \"LoftQ/Mistral-7B-v0.1-4bit-64rank\"\n",
    "\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_ID, \n",
    "    dtype=torch.bfloat16,  # you may change it with different models\n",
    "    # quantization_config=BitsAndBytesConfig(\n",
    "    #    load_in_4bit=True,\n",
    "    #    bnb_4bit_compute_dtype=torch.bfloat16, \n",
    "    #    bnb_4bit_use_double_quant=False,\n",
    "    #    bnb_4bit_quant_type='nf4',\n",
    "    # ),\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"mistralai/Mistral-7B-v0.1\")\n",
    "model = prepare_model_for_kbit_training(base_model) # TODO: Appears to be bugged. Fix it\n",
    "peft_model = PeftModel.from_pretrained(\n",
    "    model,\n",
    "    MODEL_ID,\n",
    "    subfolder=\"loftq_init\",\n",
    "    is_trainable=True\n",
    ")\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\"\n",
    "tokenizer.save_pretrained(\"./tokenizers/mistral7b64rank\", from_pt=True)\n",
    "model.save_pretrained(\"./models/mistral7b64rank\", from_pt=True)\n",
    "peft_model.save_pretrained(\"./peftmodels/mistral7b64rank\", from_pt=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4d8a91c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'type': 'MATH_AnsAug', 'query': \"Gracie and Joe are choosing numbers on the complex plane. Joe chooses the point $1+2i$. Gracie chooses $-1+i$. How far apart are Gracie and Joe's points?\", 'original_question': \"Gracie and Joe are choosing numbers on the complex plane. Joe chooses the point $1+2i$. Gracie chooses $-1+i$. How far apart are Gracie and Joe's points?\", 'response': \"The distance between two points $(x_1,y_1)$ and $(x_2,y_2)$ in the complex plane is given by the formula $\\\\sqrt{(x_2-x_1)^2+(y_2-y_1)^2}$.\\nIn this case, Joe's point is $(1,2)$ and Gracie's point is $(-1,1)$.\\nSo the distance between their points is $\\\\sqrt{((-1)-(1))^2+((1)-(2))^2}=\\\\sqrt{(-2)^2+(-1)^2}=\\\\sqrt{4+1}=\\\\sqrt{5}$.\\nTherefore, Gracie and Joe's points are $\\\\boxed{\\\\sqrt{5}}$ units apart.\\nThe answer is: \\\\sqrt{5}\"}\n"
     ]
    }
   ],
   "source": [
    "# Load MetaMathQA dataset\n",
    "dataset = load_dataset(\"meta-math/MetaMathQA\")\n",
    "\n",
    "# Take a subset for faster experimentation (optional)\n",
    "train_dataset = dataset[\"train\"].select(range(10000))  # Adjust size as needed\n",
    "\n",
    "# Preview the data\n",
    "print(train_dataset[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26c94e05",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval = dataset[\"train\"].select(range(150000, 160000))\n",
    "eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "57b6f435",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_instruction(sample):\n",
    "    \"\"\"Format the data into a prompt template\"\"\"\n",
    "    return f\"\"\"Below is a math problem. Write a response that appropriately solves the problem.\n",
    "\n",
    "    ### Problem:\n",
    "    {sample['query']}\n",
    "\n",
    "    ### Solution:\n",
    "    {sample['response']}\"\"\"\n",
    "\n",
    "def format_instruction_eval(sample):\n",
    "    \"\"\"Format the data into a prompt template\"\"\"\n",
    "    return f\"\"\"Below is a math problem. Write a response that appropriately solves the problem.\n",
    "\n",
    "    ### Problem:\n",
    "    {sample['query']}\n",
    "\n",
    "    ### Solution:\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4f573434",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 167,772,160 || all params: 7,409,504,256 || trainable%: 2.2643\n"
     ]
    }
   ],
   "source": [
    "peft_model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a6ee2ee7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ell/playground/env/lib/python3.10/site-packages/torch/backends/__init__.py:46: UserWarning: Please use the new API settings to control TF32 behavior, such as torch.backends.cudnn.conv.fp32_precision = 'tf32' or torch.backends.cuda.matmul.fp32_precision = 'ieee'. Old settings, e.g, torch.backends.cuda.matmul.allow_tf32 = True, torch.backends.cudnn.allow_tf32 = True, allowTF32CuDNN() and allowTF32CuBLAS() will be deprecated after Pytorch 2.9. Please see https://pytorch.org/docs/main/notes/cuda.html#tensorfloat-32-tf32-on-ampere-and-later-devices (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:80.)\n",
      "  self.setter(val)\n"
     ]
    }
   ],
   "source": [
    "from transformers import TrainingArguments\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./metamath-qlora\",\n",
    "    num_train_epochs=1,\n",
    "    per_device_train_batch_size=4,\n",
    "    gradient_accumulation_steps=4,     # Effective batch size = 16\n",
    "    gradient_checkpointing=True,       # Reduces memory usage\n",
    "    optim=\"paged_adamw_32bit\",        # Optimizer for QLoRA\n",
    "    logging_steps=10,\n",
    "    save_strategy=\"epoch\",\n",
    "    learning_rate=2e-4,\n",
    "    bf16=True,                         # Use bfloat16\n",
    "    tf32=True,\n",
    "    max_grad_norm=0.3,\n",
    "    warmup_ratio=0.03,\n",
    "    lr_scheduler_type=\"constant\",\n",
    "    disable_tqdm=False,\n",
    "    report_to=\"tensorboard\"                   # or \"wandb\" if you use it\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1c275fc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'pad_token_id': 2}.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='625' max='625' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [625/625 3:06:10, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.697500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.558100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>0.524300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>0.492900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>0.484500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>0.479900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>0.469400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>0.444800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>0.460900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.460200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>110</td>\n",
       "      <td>0.458200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>120</td>\n",
       "      <td>0.448000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>130</td>\n",
       "      <td>0.470300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>140</td>\n",
       "      <td>0.472300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>0.435300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>160</td>\n",
       "      <td>0.462300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>170</td>\n",
       "      <td>0.446800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>180</td>\n",
       "      <td>0.466700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>190</td>\n",
       "      <td>0.433900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.459000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>210</td>\n",
       "      <td>0.420200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>220</td>\n",
       "      <td>0.423100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>230</td>\n",
       "      <td>0.442600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>240</td>\n",
       "      <td>0.446100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>0.440600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>260</td>\n",
       "      <td>0.435500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>270</td>\n",
       "      <td>0.437000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>280</td>\n",
       "      <td>0.432100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>290</td>\n",
       "      <td>0.442000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.440500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>310</td>\n",
       "      <td>0.413200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>320</td>\n",
       "      <td>0.422100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>330</td>\n",
       "      <td>0.430200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>340</td>\n",
       "      <td>0.430900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>350</td>\n",
       "      <td>0.433700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>360</td>\n",
       "      <td>0.422500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>370</td>\n",
       "      <td>0.413100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>380</td>\n",
       "      <td>0.426200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>390</td>\n",
       "      <td>0.429300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.425900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>410</td>\n",
       "      <td>0.411100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>420</td>\n",
       "      <td>0.409900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>430</td>\n",
       "      <td>0.419700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>440</td>\n",
       "      <td>0.409300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>450</td>\n",
       "      <td>0.417100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>460</td>\n",
       "      <td>0.420100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>470</td>\n",
       "      <td>0.426200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>480</td>\n",
       "      <td>0.424700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>490</td>\n",
       "      <td>0.439200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.440700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>510</td>\n",
       "      <td>0.431700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>520</td>\n",
       "      <td>0.405300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>530</td>\n",
       "      <td>0.427900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>540</td>\n",
       "      <td>0.399100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>550</td>\n",
       "      <td>0.437800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>560</td>\n",
       "      <td>0.405800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>570</td>\n",
       "      <td>0.417200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>580</td>\n",
       "      <td>0.411900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>590</td>\n",
       "      <td>0.419000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.401600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>610</td>\n",
       "      <td>0.390600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>620</td>\n",
       "      <td>0.400800</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "('./metamath-qlora-final2/tokenizer_config.json',\n",
       " './metamath-qlora-final2/special_tokens_map.json',\n",
       " './metamath-qlora-final2/tokenizer.json')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from trl import SFTTrainer\n",
    "\n",
    "# Initialize trainer\n",
    "trainer = SFTTrainer(\n",
    "    model=peft_model,\n",
    "    train_dataset=train_dataset,\n",
    "    processing_class=tokenizer,\n",
    "    args=training_args,\n",
    "    formatting_func=format_instruction\n",
    ")\n",
    "\n",
    "# Start training\n",
    "trainer.train()\n",
    "\n",
    "trainer.model.save_pretrained(\"./metamath-qlora-final2\")\n",
    "tokenizer.save_pretrained(\"./metamath-qlora-final2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "343f9f3a",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'trainer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39msave_pretrained(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m./metamath-qlora-final2\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      2\u001b[0m tokenizer\u001b[38;5;241m.\u001b[39msave_pretrained(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m./metamath-qlora-final2\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'trainer' is not defined"
     ]
    }
   ],
   "source": [
    "trainer.model.save_pretrained(\"./metamath-qlora-final2\")\n",
    "tokenizer.save_pretrained(\"./metamath-qlora-final2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd6fae10",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(\"./metamath-qlora-final\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"./metamath-qlora-final\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1df97b99",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re \n",
    "def extract_answer(text):\n",
    "    \"\"\"\n",
    "    Extract the final numerical answer from generated text.\n",
    "    Looks for patterns like: \"The answer is X\" or boxed answers.\n",
    "    \n",
    "    Args:\n",
    "        text (str): Generated response text\n",
    "        \n",
    "    Returns:\n",
    "        str: Extracted answer or the last number found\n",
    "    \"\"\"\n",
    "    # Common patterns in math solutions\n",
    "    patterns = [\n",
    "        r\"####\\s*([+-]?\\d+\\.?\\d*)\",           # #### 42 format\n",
    "        r\"the answer is[:\\s]+([+-]?\\d+\\.?\\d*)\", # \"the answer is: 42\"\n",
    "        r\"\\\\boxed\\{([^}]+)\\}\",                 # LaTeX \\boxed{42}\n",
    "        r\"answer:\\s*([+-]?\\d+\\.?\\d*)\",         # \"answer: 42\"\n",
    "    ]\n",
    "    \n",
    "    text_lower = text.lower()\n",
    "    \n",
    "    for pattern in patterns:\n",
    "        match = re.search(pattern, text_lower)\n",
    "        if match:\n",
    "            return match.group(1).strip()\n",
    "    \n",
    "    # Fallback: return last number in text\n",
    "    numbers = re.findall(r\"[+-]?\\d+\\.?\\d*\", text)\n",
    "    if numbers:\n",
    "        return numbers[-1]\n",
    "    \n",
    "    return \"NO_ANSWER\"\n",
    "\n",
    "def extract_ground_truth(response):\n",
    "    \"\"\"\n",
    "    Extract ground truth answer from the dataset response.\n",
    "    MetaMathQA uses #### to mark the final answer.\n",
    "    \n",
    "    Args:\n",
    "        response (str): Ground truth response from dataset\n",
    "        \n",
    "    Returns:\n",
    "        str: The correct answer\n",
    "    \"\"\"\n",
    "    match = re.search(r\"####\\s*([+-]?\\d+\\.?\\d*)\", response)\n",
    "    if match:\n",
    "        return match.group(1).strip()\n",
    "    \n",
    "    # Fallback: return last number\n",
    "    numbers = re.findall(r\"[+-]?\\d+\\.?\\d*\", response)\n",
    "    if numbers:\n",
    "        return numbers[-1]\n",
    "    \n",
    "    return \"NO_ANSWER\"\n",
    "\n",
    "def compare_answers(predicted, ground_truth):\n",
    "    \"\"\"\n",
    "    Compare predicted answer with ground truth.\n",
    "    Handles numerical comparison with some tolerance for floats.\n",
    "    \n",
    "    Args:\n",
    "        predicted (str): Predicted answer\n",
    "        ground_truth (str): Correct answer\n",
    "        \n",
    "    Returns:\n",
    "        bool: True if answers match\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Try numerical comparison\n",
    "        pred_num = float(predicted)\n",
    "        truth_num = float(ground_truth)\n",
    "        \n",
    "        # Allow small floating point differences\n",
    "        return abs(pred_num - truth_num) < 0.01\n",
    "    except (ValueError, TypeError):\n",
    "        # Fall back to string comparison\n",
    "        return predicted.strip() == ground_truth.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1be005a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(\"Starting evaluation...\\n\")\n",
    "NUM_SAMPLES = 100        # How many validation samples to test (set to None for all)\n",
    "MAX_NEW_TOKENS = 256     # Maximum length of generated answer\n",
    "TEMPERATURE = 0.1        # Lower = more deterministic (good for math)\n",
    "BATCH_SIZE = 1           # Process one at a time for simplicity\n",
    "\n",
    "results = []\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "# Iterate through validation samples\n",
    "for idx, sample in enumerate(tqdm(eval, desc=\"Evaluating\")):\n",
    "    \n",
    "    # Get question and ground truth\n",
    "    question = sample[\"query\"]\n",
    "    ground_truth_response = sample[\"response\"]\n",
    "    ground_truth_answer = extract_ground_truth(ground_truth_response)\n",
    "    \n",
    "    # Format prompt (same as training!)\n",
    "    prompt = format_instruction_eval(sample)\n",
    "    \n",
    "    # Tokenize input\n",
    "    inputs = tokenizer(\n",
    "        prompt,\n",
    "        return_tensors=\"pt\",      # Return PyTorch tensors\n",
    "        truncation=True,          # Truncate if too long\n",
    "        max_length=512,           # Match training max length\n",
    "    ).to(model.device)\n",
    "    \n",
    "    # Generate answer\n",
    "    with torch.no_grad():  # Don't compute gradients (saves memory)\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=MAX_NEW_TOKENS,  # Maximum tokens to generate\n",
    "            temperature=TEMPERATURE,         # Sampling temperature (lower = more deterministic)\n",
    "            do_sample=True if TEMPERATURE > 0 else False,  # Use sampling if temp > 0\n",
    "            top_p=0.95,                     # Nucleus sampling\n",
    "            repetition_penalty=1.1,         # Penalize repetition\n",
    "            pad_token_id=tokenizer.eos_token_id,  # Padding token\n",
    "        )\n",
    "    \n",
    "    # Decode generated text\n",
    "    generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    \n",
    "    # Extract just the generated answer (remove the prompt)\n",
    "    generated_answer_text = generated_text[len(prompt):].strip()\n",
    "    \n",
    "    # Extract numerical answer from generated text\n",
    "    predicted_answer = extract_answer(generated_answer_text)\n",
    "    \n",
    "    # Compare with ground truth\n",
    "    is_correct = compare_answers(predicted_answer, ground_truth_answer)\n",
    "    \n",
    "    if is_correct:\n",
    "        correct += 1\n",
    "    total += 1\n",
    "    \n",
    "    # Store result\n",
    "    results.append({\n",
    "        \"question\": question,\n",
    "        \"predicted_answer\": predicted_answer,\n",
    "        \"ground_truth_answer\": ground_truth_answer,\n",
    "        \"correct\": is_correct,\n",
    "        \"full_generation\": generated_answer_text,\n",
    "    })\n",
    "    \n",
    "    # Print first few examples for inspection\n",
    "    if idx < 3:\n",
    "        print(f\"\\n{'='*80}\")\n",
    "        print(f\"Example {idx + 1}:\")\n",
    "        print(f\"\\nQuestion: {question}\")\n",
    "        print(f\"\\nGenerated: {generated_answer_text[:200]}...\")\n",
    "        print(f\"\\nPredicted Answer: {predicted_answer}\")\n",
    "        print(f\"Ground Truth: {ground_truth_answer}\")\n",
    "        print(f\"Correct: {'✓' if is_correct else '✗'}\")\n",
    "        print(f\"{'='*80}\\n\")\n",
    "\n",
    "# ==============================================================================\n",
    "# CALCULATE METRICS\n",
    "# ==============================================================================\n",
    "\n",
    "accuracy = (correct / total) * 100 if total > 0 else 0\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"EVALUATION RESULTS\")\n",
    "print(\"=\"*80)\n",
    "print(f\"Total samples evaluated: {total}\")\n",
    "print(f\"Correct answers: {correct}\")\n",
    "print(f\"Incorrect answers: {total - correct}\")\n",
    "print(f\"Accuracy: {accuracy:.2f}%\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# ==============================================================================\n",
    "# SAVE RESULTS\n",
    "# ==============================================================================\n",
    "\n",
    "import json\n",
    "\n",
    "output_file = \"evaluation_results.json\"\n",
    "with open(output_file, \"w\") as f:\n",
    "    json.dump({\n",
    "        \"summary\": {\n",
    "            \"total\": total,\n",
    "            \"correct\": correct,\n",
    "            \"accuracy\": accuracy,\n",
    "        },\n",
    "        \"results\": results,\n",
    "    }, f, indent=2)\n",
    "\n",
    "print(f\"\\nDetailed results saved to: {output_file}\")\n",
    "\n",
    "# ==============================================================================\n",
    "# ERROR ANALYSIS (Optional)\n",
    "# ==============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"ERROR ANALYSIS - Sample Wrong Answers\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "wrong_answers = [r for r in results if not r[\"correct\"]][:5]  # Show first 5 errors\n",
    "\n",
    "for i, error in enumerate(wrong_answers, 1):\n",
    "    print(f\"\\nError {i}:\")\n",
    "    print(f\"Question: {error['question'][:100]}...\")\n",
    "    print(f\"Predicted: {error['predicted_answer']}\")\n",
    "    print(f\"Correct: {error['ground_truth_answer']}\")\n",
    "    print(\"-\" * 40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64b7429a",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = tokenizer(format_instruction_eval(train_dataset[0]), return_tensors=\"pt\").to(\"cuda\")\n",
    "output = model.generate(**inputs)\n",
    "\n",
    "print(tokenizer.decode(output[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8dcc6a93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 11-13 22:35:33 [__init__.py:216] Automatically detected platform cuda.\n"
     ]
    }
   ],
   "source": [
    "from vllm import LLM, SamplingParams\n",
    "from vllm.lora.request import LoRARequest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "366b829a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 11-14 00:37:47 [utils.py:233] non-default args: {'tokenizer': './tokenizers/mistral7b64rank', 'trust_remote_code': True, 'disable_log_stats': True, 'model': './models/mistral7b64rank'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 11-14 00:37:47 [model.py:547] Resolved architecture: MistralForCausalLM\n",
      "INFO 11-14 00:37:47 [model.py:1730] Downcasting torch.float32 to torch.bfloat16.\n",
      "INFO 11-14 00:37:47 [model.py:1510] Using max model len 32768\n",
      "INFO 11-14 00:37:47 [scheduler.py:205] Chunked prefill is enabled with max_num_batched_tokens=8192.\n",
      "\u001b[1;36m(EngineCore_DP0 pid=38199)\u001b[0;0m INFO 11-14 00:37:47 [core.py:644] Waiting for init message from front-end.\n",
      "\u001b[1;36m(EngineCore_DP0 pid=38199)\u001b[0;0m INFO 11-14 00:37:47 [core.py:77] Initializing a V1 LLM engine (v0.11.0) with config: model='./models/mistral7b64rank', speculative_config=None, tokenizer='./tokenizers/mistral7b64rank', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=32768, download_dir=None, load_format=bitsandbytes, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=False, quantization=bitsandbytes, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=./models/mistral7b64rank, enable_prefix_caching=True, chunked_prefill_enabled=True, pooler_config=None, compilation_config={\"level\":3,\"debug_dump_path\":\"\",\"cache_dir\":\"\",\"backend\":\"\",\"custom_ops\":[],\"splitting_ops\":[\"vllm.unified_attention\",\"vllm.unified_attention_with_output\",\"vllm.mamba_mixer2\",\"vllm.mamba_mixer\",\"vllm.short_conv\",\"vllm.linear_attention\",\"vllm.plamo2_mamba_mixer\",\"vllm.gdn_attention\",\"vllm.sparse_attn_indexer\"],\"use_inductor\":true,\"compile_sizes\":[],\"inductor_compile_config\":{\"enable_auto_functionalized_v2\":false},\"inductor_passes\":{},\"cudagraph_mode\":[2,1],\"use_cudagraph\":true,\"cudagraph_num_of_warmups\":1,\"cudagraph_capture_sizes\":[512,504,496,488,480,472,464,456,448,440,432,424,416,408,400,392,384,376,368,360,352,344,336,328,320,312,304,296,288,280,272,264,256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],\"cudagraph_copy_inputs\":false,\"full_cuda_graph\":false,\"use_inductor_graph_partition\":false,\"pass_config\":{},\"max_capture_size\":512,\"local_cache_dir\":null}\n",
      "\u001b[1;36m(EngineCore_DP0 pid=38199)\u001b[0;0m WARNING 11-14 00:37:48 [interface.py:381] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.\n",
      "\u001b[1;36m(EngineCore_DP0 pid=38199)\u001b[0;0m INFO 11-14 00:37:51 [parallel_state.py:1208] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "\u001b[1;36m(EngineCore_DP0 pid=38199)\u001b[0;0m WARNING 11-14 00:37:54 [topk_topp_sampler.py:66] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "\u001b[1;36m(EngineCore_DP0 pid=38199)\u001b[0;0m INFO 11-14 00:37:54 [gpu_model_runner.py:2602] Starting to load model ./models/mistral7b64rank...\n",
      "\u001b[1;36m(EngineCore_DP0 pid=38199)\u001b[0;0m INFO 11-14 00:37:54 [gpu_model_runner.py:2634] Loading model from scratch...\n",
      "\u001b[1;36m(EngineCore_DP0 pid=38199)\u001b[0;0m INFO 11-14 00:37:55 [cuda.py:366] Using Flash Attention backend on V1 engine.\n",
      "\u001b[1;36m(EngineCore_DP0 pid=38199)\u001b[0;0m INFO 11-14 00:37:55 [bitsandbytes_loader.py:759] Loading weights with BitsAndBytes quantization. May take a while ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:01<00:00,  1.66it/s]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:01<00:00,  1.66it/s]\n",
      "\u001b[1;36m(EngineCore_DP0 pid=38199)\u001b[0;0m \n",
      "Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(EngineCore_DP0 pid=38199)\u001b[0;0m ERROR 11-14 00:37:57 [core.py:708] EngineCore failed to start.\n",
      "\u001b[1;36m(EngineCore_DP0 pid=38199)\u001b[0;0m ERROR 11-14 00:37:57 [core.py:708] Traceback (most recent call last):\n",
      "\u001b[1;36m(EngineCore_DP0 pid=38199)\u001b[0;0m ERROR 11-14 00:37:57 [core.py:708]   File \"/home/ell/playground/env/lib/python3.10/site-packages/vllm/v1/engine/core.py\", line 699, in run_engine_core\n",
      "\u001b[1;36m(EngineCore_DP0 pid=38199)\u001b[0;0m ERROR 11-14 00:37:57 [core.py:708]     engine_core = EngineCoreProc(*args, **kwargs)\n",
      "\u001b[1;36m(EngineCore_DP0 pid=38199)\u001b[0;0m ERROR 11-14 00:37:57 [core.py:708]   File \"/home/ell/playground/env/lib/python3.10/site-packages/vllm/v1/engine/core.py\", line 498, in __init__\n",
      "\u001b[1;36m(EngineCore_DP0 pid=38199)\u001b[0;0m ERROR 11-14 00:37:57 [core.py:708]     super().__init__(vllm_config, executor_class, log_stats,\n",
      "\u001b[1;36m(EngineCore_DP0 pid=38199)\u001b[0;0m ERROR 11-14 00:37:57 [core.py:708]   File \"/home/ell/playground/env/lib/python3.10/site-packages/vllm/v1/engine/core.py\", line 83, in __init__\n",
      "\u001b[1;36m(EngineCore_DP0 pid=38199)\u001b[0;0m ERROR 11-14 00:37:57 [core.py:708]     self.model_executor = executor_class(vllm_config)\n",
      "\u001b[1;36m(EngineCore_DP0 pid=38199)\u001b[0;0m ERROR 11-14 00:37:57 [core.py:708]   File \"/home/ell/playground/env/lib/python3.10/site-packages/vllm/executor/executor_base.py\", line 54, in __init__\n",
      "\u001b[1;36m(EngineCore_DP0 pid=38199)\u001b[0;0m ERROR 11-14 00:37:57 [core.py:708]     self._init_executor()\n",
      "\u001b[1;36m(EngineCore_DP0 pid=38199)\u001b[0;0m ERROR 11-14 00:37:57 [core.py:708]   File \"/home/ell/playground/env/lib/python3.10/site-packages/vllm/executor/uniproc_executor.py\", line 55, in _init_executor\n",
      "\u001b[1;36m(EngineCore_DP0 pid=38199)\u001b[0;0m ERROR 11-14 00:37:57 [core.py:708]     self.collective_rpc(\"load_model\")\n",
      "\u001b[1;36m(EngineCore_DP0 pid=38199)\u001b[0;0m ERROR 11-14 00:37:57 [core.py:708]   File \"/home/ell/playground/env/lib/python3.10/site-packages/vllm/executor/uniproc_executor.py\", line 83, in collective_rpc\n",
      "\u001b[1;36m(EngineCore_DP0 pid=38199)\u001b[0;0m ERROR 11-14 00:37:57 [core.py:708]     return [run_method(self.driver_worker, method, args, kwargs)]\n",
      "\u001b[1;36m(EngineCore_DP0 pid=38199)\u001b[0;0m ERROR 11-14 00:37:57 [core.py:708]   File \"/home/ell/playground/env/lib/python3.10/site-packages/vllm/utils/__init__.py\", line 3122, in run_method\n",
      "\u001b[1;36m(EngineCore_DP0 pid=38199)\u001b[0;0m ERROR 11-14 00:37:57 [core.py:708]     return func(*args, **kwargs)\n",
      "\u001b[1;36m(EngineCore_DP0 pid=38199)\u001b[0;0m ERROR 11-14 00:37:57 [core.py:708]   File \"/home/ell/playground/env/lib/python3.10/site-packages/vllm/v1/worker/gpu_worker.py\", line 213, in load_model\n",
      "\u001b[1;36m(EngineCore_DP0 pid=38199)\u001b[0;0m ERROR 11-14 00:37:57 [core.py:708]     self.model_runner.load_model(eep_scale_up=eep_scale_up)\n",
      "\u001b[1;36m(EngineCore_DP0 pid=38199)\u001b[0;0m ERROR 11-14 00:37:57 [core.py:708]   File \"/home/ell/playground/env/lib/python3.10/site-packages/vllm/v1/worker/gpu_model_runner.py\", line 2635, in load_model\n",
      "\u001b[1;36m(EngineCore_DP0 pid=38199)\u001b[0;0m ERROR 11-14 00:37:57 [core.py:708]     self.model = model_loader.load_model(\n",
      "\u001b[1;36m(EngineCore_DP0 pid=38199)\u001b[0;0m ERROR 11-14 00:37:57 [core.py:708]   File \"/home/ell/playground/env/lib/python3.10/site-packages/vllm/model_executor/model_loader/base_loader.py\", line 50, in load_model\n",
      "\u001b[1;36m(EngineCore_DP0 pid=38199)\u001b[0;0m ERROR 11-14 00:37:57 [core.py:708]     self.load_weights(model, model_config)\n",
      "\u001b[1;36m(EngineCore_DP0 pid=38199)\u001b[0;0m ERROR 11-14 00:37:57 [core.py:708]   File \"/home/ell/playground/env/lib/python3.10/site-packages/vllm/model_executor/model_loader/bitsandbytes_loader.py\", line 767, in load_weights\n",
      "\u001b[1;36m(EngineCore_DP0 pid=38199)\u001b[0;0m ERROR 11-14 00:37:57 [core.py:708]     loaded_weights = model.load_weights(qweight_iterator)\n",
      "\u001b[1;36m(EngineCore_DP0 pid=38199)\u001b[0;0m ERROR 11-14 00:37:57 [core.py:708]   File \"/home/ell/playground/env/lib/python3.10/site-packages/vllm/model_executor/models/llama.py\", line 615, in load_weights\n",
      "\u001b[1;36m(EngineCore_DP0 pid=38199)\u001b[0;0m ERROR 11-14 00:37:57 [core.py:708]     return loader.load_weights(\n",
      "\u001b[1;36m(EngineCore_DP0 pid=38199)\u001b[0;0m ERROR 11-14 00:37:57 [core.py:708]   File \"/home/ell/playground/env/lib/python3.10/site-packages/vllm/model_executor/models/utils.py\", line 294, in load_weights\n",
      "\u001b[1;36m(EngineCore_DP0 pid=38199)\u001b[0;0m ERROR 11-14 00:37:57 [core.py:708]     autoloaded_weights = set(self._load_module(\"\", self.module, weights))\n",
      "\u001b[1;36m(EngineCore_DP0 pid=38199)\u001b[0;0m ERROR 11-14 00:37:57 [core.py:708]   File \"/home/ell/playground/env/lib/python3.10/site-packages/vllm/model_executor/models/utils.py\", line 252, in _load_module\n",
      "\u001b[1;36m(EngineCore_DP0 pid=38199)\u001b[0;0m ERROR 11-14 00:37:57 [core.py:708]     yield from self._load_module(prefix,\n",
      "\u001b[1;36m(EngineCore_DP0 pid=38199)\u001b[0;0m ERROR 11-14 00:37:57 [core.py:708]   File \"/home/ell/playground/env/lib/python3.10/site-packages/vllm/model_executor/models/utils.py\", line 225, in _load_module\n",
      "\u001b[1;36m(EngineCore_DP0 pid=38199)\u001b[0;0m ERROR 11-14 00:37:57 [core.py:708]     loaded_params = module_load_weights(weights)\n",
      "\u001b[1;36m(EngineCore_DP0 pid=38199)\u001b[0;0m ERROR 11-14 00:37:57 [core.py:708]   File \"/home/ell/playground/env/lib/python3.10/site-packages/vllm/model_executor/models/llama.py\", line 478, in load_weights\n",
      "\u001b[1;36m(EngineCore_DP0 pid=38199)\u001b[0;0m ERROR 11-14 00:37:57 [core.py:708]     param = params_dict[name]\n",
      "\u001b[1;36m(EngineCore_DP0 pid=38199)\u001b[0;0m ERROR 11-14 00:37:57 [core.py:708] KeyError: 'layers.31.mlp.down_proj.base_layer.weight'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(EngineCore_DP0 pid=38199)\u001b[0;0m Process EngineCore_DP0:\n",
      "\u001b[1;36m(EngineCore_DP0 pid=38199)\u001b[0;0m Traceback (most recent call last):\n",
      "\u001b[1;36m(EngineCore_DP0 pid=38199)\u001b[0;0m   File \"/usr/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "\u001b[1;36m(EngineCore_DP0 pid=38199)\u001b[0;0m     self.run()\n",
      "\u001b[1;36m(EngineCore_DP0 pid=38199)\u001b[0;0m   File \"/usr/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "\u001b[1;36m(EngineCore_DP0 pid=38199)\u001b[0;0m     self._target(*self._args, **self._kwargs)\n",
      "\u001b[1;36m(EngineCore_DP0 pid=38199)\u001b[0;0m   File \"/home/ell/playground/env/lib/python3.10/site-packages/vllm/v1/engine/core.py\", line 712, in run_engine_core\n",
      "\u001b[1;36m(EngineCore_DP0 pid=38199)\u001b[0;0m     raise e\n",
      "\u001b[1;36m(EngineCore_DP0 pid=38199)\u001b[0;0m   File \"/home/ell/playground/env/lib/python3.10/site-packages/vllm/v1/engine/core.py\", line 699, in run_engine_core\n",
      "\u001b[1;36m(EngineCore_DP0 pid=38199)\u001b[0;0m     engine_core = EngineCoreProc(*args, **kwargs)\n",
      "\u001b[1;36m(EngineCore_DP0 pid=38199)\u001b[0;0m   File \"/home/ell/playground/env/lib/python3.10/site-packages/vllm/v1/engine/core.py\", line 498, in __init__\n",
      "\u001b[1;36m(EngineCore_DP0 pid=38199)\u001b[0;0m     super().__init__(vllm_config, executor_class, log_stats,\n",
      "\u001b[1;36m(EngineCore_DP0 pid=38199)\u001b[0;0m   File \"/home/ell/playground/env/lib/python3.10/site-packages/vllm/v1/engine/core.py\", line 83, in __init__\n",
      "\u001b[1;36m(EngineCore_DP0 pid=38199)\u001b[0;0m     self.model_executor = executor_class(vllm_config)\n",
      "\u001b[1;36m(EngineCore_DP0 pid=38199)\u001b[0;0m   File \"/home/ell/playground/env/lib/python3.10/site-packages/vllm/executor/executor_base.py\", line 54, in __init__\n",
      "\u001b[1;36m(EngineCore_DP0 pid=38199)\u001b[0;0m     self._init_executor()\n",
      "\u001b[1;36m(EngineCore_DP0 pid=38199)\u001b[0;0m   File \"/home/ell/playground/env/lib/python3.10/site-packages/vllm/executor/uniproc_executor.py\", line 55, in _init_executor\n",
      "\u001b[1;36m(EngineCore_DP0 pid=38199)\u001b[0;0m     self.collective_rpc(\"load_model\")\n",
      "\u001b[1;36m(EngineCore_DP0 pid=38199)\u001b[0;0m   File \"/home/ell/playground/env/lib/python3.10/site-packages/vllm/executor/uniproc_executor.py\", line 83, in collective_rpc\n",
      "\u001b[1;36m(EngineCore_DP0 pid=38199)\u001b[0;0m     return [run_method(self.driver_worker, method, args, kwargs)]\n",
      "\u001b[1;36m(EngineCore_DP0 pid=38199)\u001b[0;0m   File \"/home/ell/playground/env/lib/python3.10/site-packages/vllm/utils/__init__.py\", line 3122, in run_method\n",
      "\u001b[1;36m(EngineCore_DP0 pid=38199)\u001b[0;0m     return func(*args, **kwargs)\n",
      "\u001b[1;36m(EngineCore_DP0 pid=38199)\u001b[0;0m   File \"/home/ell/playground/env/lib/python3.10/site-packages/vllm/v1/worker/gpu_worker.py\", line 213, in load_model\n",
      "\u001b[1;36m(EngineCore_DP0 pid=38199)\u001b[0;0m     self.model_runner.load_model(eep_scale_up=eep_scale_up)\n",
      "\u001b[1;36m(EngineCore_DP0 pid=38199)\u001b[0;0m   File \"/home/ell/playground/env/lib/python3.10/site-packages/vllm/v1/worker/gpu_model_runner.py\", line 2635, in load_model\n",
      "\u001b[1;36m(EngineCore_DP0 pid=38199)\u001b[0;0m     self.model = model_loader.load_model(\n",
      "\u001b[1;36m(EngineCore_DP0 pid=38199)\u001b[0;0m   File \"/home/ell/playground/env/lib/python3.10/site-packages/vllm/model_executor/model_loader/base_loader.py\", line 50, in load_model\n",
      "\u001b[1;36m(EngineCore_DP0 pid=38199)\u001b[0;0m     self.load_weights(model, model_config)\n",
      "\u001b[1;36m(EngineCore_DP0 pid=38199)\u001b[0;0m   File \"/home/ell/playground/env/lib/python3.10/site-packages/vllm/model_executor/model_loader/bitsandbytes_loader.py\", line 767, in load_weights\n",
      "\u001b[1;36m(EngineCore_DP0 pid=38199)\u001b[0;0m     loaded_weights = model.load_weights(qweight_iterator)\n",
      "\u001b[1;36m(EngineCore_DP0 pid=38199)\u001b[0;0m   File \"/home/ell/playground/env/lib/python3.10/site-packages/vllm/model_executor/models/llama.py\", line 615, in load_weights\n",
      "\u001b[1;36m(EngineCore_DP0 pid=38199)\u001b[0;0m     return loader.load_weights(\n",
      "\u001b[1;36m(EngineCore_DP0 pid=38199)\u001b[0;0m   File \"/home/ell/playground/env/lib/python3.10/site-packages/vllm/model_executor/models/utils.py\", line 294, in load_weights\n",
      "\u001b[1;36m(EngineCore_DP0 pid=38199)\u001b[0;0m     autoloaded_weights = set(self._load_module(\"\", self.module, weights))\n",
      "\u001b[1;36m(EngineCore_DP0 pid=38199)\u001b[0;0m   File \"/home/ell/playground/env/lib/python3.10/site-packages/vllm/model_executor/models/utils.py\", line 252, in _load_module\n",
      "\u001b[1;36m(EngineCore_DP0 pid=38199)\u001b[0;0m     yield from self._load_module(prefix,\n",
      "\u001b[1;36m(EngineCore_DP0 pid=38199)\u001b[0;0m   File \"/home/ell/playground/env/lib/python3.10/site-packages/vllm/model_executor/models/utils.py\", line 225, in _load_module\n",
      "\u001b[1;36m(EngineCore_DP0 pid=38199)\u001b[0;0m     loaded_params = module_load_weights(weights)\n",
      "\u001b[1;36m(EngineCore_DP0 pid=38199)\u001b[0;0m   File \"/home/ell/playground/env/lib/python3.10/site-packages/vllm/model_executor/models/llama.py\", line 478, in load_weights\n",
      "\u001b[1;36m(EngineCore_DP0 pid=38199)\u001b[0;0m     param = params_dict[name]\n",
      "\u001b[1;36m(EngineCore_DP0 pid=38199)\u001b[0;0m KeyError: 'layers.31.mlp.down_proj.base_layer.weight'\n",
      "Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:01<?, ?it/s]\n",
      "\u001b[1;36m(EngineCore_DP0 pid=38199)\u001b[0;0m \n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Engine core initialization failed. See root cause above. Failed core proc(s): {}",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m llm \u001b[38;5;241m=\u001b[39m \u001b[43mLLM\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m./models/mistral7b64rank\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m./tokenizers/mistral7b64rank\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/playground/env/lib/python3.10/site-packages/vllm/entrypoints/llm.py:297\u001b[0m, in \u001b[0;36mLLM.__init__\u001b[0;34m(self, model, runner, convert, tokenizer, tokenizer_mode, skip_tokenizer_init, trust_remote_code, allowed_local_media_path, allowed_media_domains, tensor_parallel_size, dtype, quantization, revision, tokenizer_revision, seed, gpu_memory_utilization, swap_space, cpu_offload_gb, enforce_eager, disable_custom_all_reduce, hf_token, hf_overrides, mm_processor_kwargs, pooler_config, override_pooler_config, structured_outputs_config, kv_cache_memory_bytes, compilation_config, logits_processors, **kwargs)\u001b[0m\n\u001b[1;32m    294\u001b[0m log_non_default_args(engine_args)\n\u001b[1;32m    296\u001b[0m \u001b[38;5;66;03m# Create the Engine (autoselects V0 vs V1)\u001b[39;00m\n\u001b[0;32m--> 297\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mllm_engine \u001b[38;5;241m=\u001b[39m \u001b[43mLLMEngine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_engine_args\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    298\u001b[0m \u001b[43m    \u001b[49m\u001b[43mengine_args\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mengine_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43musage_context\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mUsageContext\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mLLM_CLASS\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    299\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mengine_class \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mllm_engine)\n\u001b[1;32m    301\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrequest_counter \u001b[38;5;241m=\u001b[39m Counter()\n",
      "File \u001b[0;32m~/playground/env/lib/python3.10/site-packages/vllm/v1/engine/llm_engine.py:177\u001b[0m, in \u001b[0;36mLLMEngine.from_engine_args\u001b[0;34m(cls, engine_args, usage_context, stat_loggers, enable_multiprocessing)\u001b[0m\n\u001b[1;32m    174\u001b[0m     enable_multiprocessing \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    176\u001b[0m \u001b[38;5;66;03m# Create the LLMEngine.\u001b[39;00m\n\u001b[0;32m--> 177\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mvllm_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvllm_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    178\u001b[0m \u001b[43m           \u001b[49m\u001b[43mexecutor_class\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexecutor_class\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    179\u001b[0m \u001b[43m           \u001b[49m\u001b[43mlog_stats\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mengine_args\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdisable_log_stats\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    180\u001b[0m \u001b[43m           \u001b[49m\u001b[43musage_context\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43musage_context\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    181\u001b[0m \u001b[43m           \u001b[49m\u001b[43mstat_loggers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstat_loggers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    182\u001b[0m \u001b[43m           \u001b[49m\u001b[43mmultiprocess_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43menable_multiprocessing\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/playground/env/lib/python3.10/site-packages/vllm/v1/engine/llm_engine.py:114\u001b[0m, in \u001b[0;36mLLMEngine.__init__\u001b[0;34m(self, vllm_config, executor_class, log_stats, usage_context, stat_loggers, mm_registry, use_cached_outputs, multiprocess_mode)\u001b[0m\n\u001b[1;32m    111\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput_processor\u001b[38;5;241m.\u001b[39mtracer \u001b[38;5;241m=\u001b[39m tracer\n\u001b[1;32m    113\u001b[0m \u001b[38;5;66;03m# EngineCore (gets EngineCoreRequests and gives EngineCoreOutputs)\u001b[39;00m\n\u001b[0;32m--> 114\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mengine_core \u001b[38;5;241m=\u001b[39m \u001b[43mEngineCoreClient\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmake_client\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    115\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmultiprocess_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmultiprocess_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    116\u001b[0m \u001b[43m    \u001b[49m\u001b[43masyncio_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    117\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvllm_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvllm_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    118\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexecutor_class\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexecutor_class\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    119\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlog_stats\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlog_stats\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    120\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    122\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlogger_manager: Optional[StatLoggerManager] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    123\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlog_stats:\n",
      "File \u001b[0;32m~/playground/env/lib/python3.10/site-packages/vllm/v1/engine/core_client.py:80\u001b[0m, in \u001b[0;36mEngineCoreClient.make_client\u001b[0;34m(multiprocess_mode, asyncio_mode, vllm_config, executor_class, log_stats)\u001b[0m\n\u001b[1;32m     76\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m EngineCoreClient\u001b[38;5;241m.\u001b[39mmake_async_mp_client(\n\u001b[1;32m     77\u001b[0m         vllm_config, executor_class, log_stats)\n\u001b[1;32m     79\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m multiprocess_mode \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m asyncio_mode:\n\u001b[0;32m---> 80\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mSyncMPClient\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvllm_config\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexecutor_class\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlog_stats\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     82\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m InprocClient(vllm_config, executor_class, log_stats)\n",
      "File \u001b[0;32m~/playground/env/lib/python3.10/site-packages/vllm/v1/engine/core_client.py:602\u001b[0m, in \u001b[0;36mSyncMPClient.__init__\u001b[0;34m(self, vllm_config, executor_class, log_stats)\u001b[0m\n\u001b[1;32m    600\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, vllm_config: VllmConfig, executor_class: \u001b[38;5;28mtype\u001b[39m[Executor],\n\u001b[1;32m    601\u001b[0m              log_stats: \u001b[38;5;28mbool\u001b[39m):\n\u001b[0;32m--> 602\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m    603\u001b[0m \u001b[43m        \u001b[49m\u001b[43masyncio_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    604\u001b[0m \u001b[43m        \u001b[49m\u001b[43mvllm_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvllm_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    605\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexecutor_class\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexecutor_class\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    606\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlog_stats\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlog_stats\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    607\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    609\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_dp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvllm_config\u001b[38;5;241m.\u001b[39mparallel_config\u001b[38;5;241m.\u001b[39mdata_parallel_size \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    610\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutputs_queue \u001b[38;5;241m=\u001b[39m queue\u001b[38;5;241m.\u001b[39mQueue[Union[EngineCoreOutputs, \u001b[38;5;167;01mException\u001b[39;00m]]()\n",
      "File \u001b[0;32m~/playground/env/lib/python3.10/site-packages/vllm/v1/engine/core_client.py:448\u001b[0m, in \u001b[0;36mMPClient.__init__\u001b[0;34m(self, asyncio_mode, vllm_config, executor_class, log_stats, client_addresses)\u001b[0m\n\u001b[1;32m    444\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstats_update_address \u001b[38;5;241m=\u001b[39m client_addresses\u001b[38;5;241m.\u001b[39mget(\n\u001b[1;32m    445\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstats_update_address\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    446\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    447\u001b[0m     \u001b[38;5;66;03m# Engines are managed by this client.\u001b[39;00m\n\u001b[0;32m--> 448\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m launch_core_engines(vllm_config, executor_class,\n\u001b[1;32m    449\u001b[0m                              log_stats) \u001b[38;5;28;01mas\u001b[39;00m (engine_manager,\n\u001b[1;32m    450\u001b[0m                                             coordinator,\n\u001b[1;32m    451\u001b[0m                                             addresses):\n\u001b[1;32m    452\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mresources\u001b[38;5;241m.\u001b[39mcoordinator \u001b[38;5;241m=\u001b[39m coordinator\n\u001b[1;32m    453\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mresources\u001b[38;5;241m.\u001b[39mengine_manager \u001b[38;5;241m=\u001b[39m engine_manager\n",
      "File \u001b[0;32m/usr/lib/python3.10/contextlib.py:142\u001b[0m, in \u001b[0;36m_GeneratorContextManager.__exit__\u001b[0;34m(self, typ, value, traceback)\u001b[0m\n\u001b[1;32m    140\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m typ \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    141\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 142\u001b[0m         \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgen\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    143\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n\u001b[1;32m    144\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[0;32m~/playground/env/lib/python3.10/site-packages/vllm/v1/engine/utils.py:732\u001b[0m, in \u001b[0;36mlaunch_core_engines\u001b[0;34m(vllm_config, executor_class, log_stats, num_api_servers)\u001b[0m\n\u001b[1;32m    729\u001b[0m \u001b[38;5;28;01myield\u001b[39;00m local_engine_manager, coordinator, addresses\n\u001b[1;32m    731\u001b[0m \u001b[38;5;66;03m# Now wait for engines to start.\u001b[39;00m\n\u001b[0;32m--> 732\u001b[0m \u001b[43mwait_for_engine_startup\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    733\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhandshake_socket\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    734\u001b[0m \u001b[43m    \u001b[49m\u001b[43maddresses\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    735\u001b[0m \u001b[43m    \u001b[49m\u001b[43mengines_to_handshake\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    736\u001b[0m \u001b[43m    \u001b[49m\u001b[43mparallel_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    737\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvllm_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcache_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    738\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlocal_engine_manager\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    739\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcoordinator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mproc\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mcoordinator\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    740\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/playground/env/lib/python3.10/site-packages/vllm/v1/engine/utils.py:785\u001b[0m, in \u001b[0;36mwait_for_engine_startup\u001b[0;34m(handshake_socket, addresses, core_engines, parallel_config, cache_config, proc_manager, coord_process)\u001b[0m\n\u001b[1;32m    783\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m coord_process \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m coord_process\u001b[38;5;241m.\u001b[39mexitcode \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    784\u001b[0m         finished[coord_process\u001b[38;5;241m.\u001b[39mname] \u001b[38;5;241m=\u001b[39m coord_process\u001b[38;5;241m.\u001b[39mexitcode\n\u001b[0;32m--> 785\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEngine core initialization failed. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    786\u001b[0m                        \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSee root cause above. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    787\u001b[0m                        \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFailed core proc(s): \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfinished\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    789\u001b[0m \u001b[38;5;66;03m# Receive HELLO and READY messages from the input socket.\u001b[39;00m\n\u001b[1;32m    790\u001b[0m eng_identity, ready_msg_bytes \u001b[38;5;241m=\u001b[39m handshake_socket\u001b[38;5;241m.\u001b[39mrecv_multipart()\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Engine core initialization failed. See root cause above. Failed core proc(s): {}"
     ]
    }
   ],
   "source": [
    "llm = LLM(model=\"./models/mistral7b64rank\", tokenizer=\"./tokenizers/mistral7b64rank\", trust_remote_code=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7f0381db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 11-13 22:37:35 [utils.py:233] non-default args: {'load_format': 'bitsandbytes', 'dtype': torch.bfloat16, 'gpu_memory_utilization': 0.5, 'disable_log_stats': True, 'quantization': 'bitsandbytes', 'model': 'unsloth/tinyllama-bnb-4bit'}\n",
      "INFO 11-13 22:37:36 [model.py:547] Resolved architecture: LlamaForCausalLM\n",
      "INFO 11-13 22:37:36 [model.py:1510] Using max model len 2048\n",
      "INFO 11-13 22:37:36 [scheduler.py:205] Chunked prefill is enabled with max_num_batched_tokens=8192.\n",
      "\u001b[1;36m(EngineCore_DP0 pid=4125)\u001b[0;0m INFO 11-13 22:37:37 [core.py:644] Waiting for init message from front-end.\n",
      "\u001b[1;36m(EngineCore_DP0 pid=4125)\u001b[0;0m INFO 11-13 22:37:37 [core.py:77] Initializing a V1 LLM engine (v0.11.0) with config: model='unsloth/tinyllama-bnb-4bit', speculative_config=None, tokenizer='unsloth/tinyllama-bnb-4bit', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=bitsandbytes, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=False, quantization=bitsandbytes, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=unsloth/tinyllama-bnb-4bit, enable_prefix_caching=True, chunked_prefill_enabled=True, pooler_config=None, compilation_config={\"level\":3,\"debug_dump_path\":\"\",\"cache_dir\":\"\",\"backend\":\"\",\"custom_ops\":[],\"splitting_ops\":[\"vllm.unified_attention\",\"vllm.unified_attention_with_output\",\"vllm.mamba_mixer2\",\"vllm.mamba_mixer\",\"vllm.short_conv\",\"vllm.linear_attention\",\"vllm.plamo2_mamba_mixer\",\"vllm.gdn_attention\",\"vllm.sparse_attn_indexer\"],\"use_inductor\":true,\"compile_sizes\":[],\"inductor_compile_config\":{\"enable_auto_functionalized_v2\":false},\"inductor_passes\":{},\"cudagraph_mode\":[2,1],\"use_cudagraph\":true,\"cudagraph_num_of_warmups\":1,\"cudagraph_capture_sizes\":[512,504,496,488,480,472,464,456,448,440,432,424,416,408,400,392,384,376,368,360,352,344,336,328,320,312,304,296,288,280,272,264,256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],\"cudagraph_copy_inputs\":false,\"full_cuda_graph\":false,\"use_inductor_graph_partition\":false,\"pass_config\":{},\"max_capture_size\":512,\"local_cache_dir\":null}\n",
      "\u001b[1;36m(EngineCore_DP0 pid=4125)\u001b[0;0m WARNING 11-13 22:37:37 [interface.py:381] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.\n",
      "\u001b[1;36m(EngineCore_DP0 pid=4125)\u001b[0;0m INFO 11-13 22:37:40 [parallel_state.py:1208] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "\u001b[1;36m(EngineCore_DP0 pid=4125)\u001b[0;0m WARNING 11-13 22:37:40 [topk_topp_sampler.py:66] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "\u001b[1;36m(EngineCore_DP0 pid=4125)\u001b[0;0m INFO 11-13 22:37:40 [gpu_model_runner.py:2602] Starting to load model unsloth/tinyllama-bnb-4bit...\n",
      "\u001b[1;36m(EngineCore_DP0 pid=4125)\u001b[0;0m INFO 11-13 22:37:40 [gpu_model_runner.py:2634] Loading model from scratch...\n",
      "\u001b[1;36m(EngineCore_DP0 pid=4125)\u001b[0;0m INFO 11-13 22:37:41 [cuda.py:366] Using Flash Attention backend on V1 engine.\n",
      "\u001b[1;36m(EngineCore_DP0 pid=4125)\u001b[0;0m INFO 11-13 22:37:41 [bitsandbytes_loader.py:759] Loading weights with BitsAndBytes quantization. May take a while ...\n",
      "\u001b[1;36m(EngineCore_DP0 pid=4125)\u001b[0;0m INFO 11-13 22:37:41 [weight_utils.py:392] Using model weights format ['*.safetensors']\n",
      "\u001b[1;36m(EngineCore_DP0 pid=4125)\u001b[0;0m INFO 11-13 22:38:06 [weight_utils.py:413] Time spent downloading weights for unsloth/tinyllama-bnb-4bit: 24.917161 seconds\n",
      "\u001b[1;36m(EngineCore_DP0 pid=4125)\u001b[0;0m INFO 11-13 22:38:06 [weight_utils.py:450] No model.safetensors.index.json found in remote.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00, 38.27it/s]\n",
      "\u001b[1;36m(EngineCore_DP0 pid=4125)\u001b[0;0m \n",
      "Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  1.74it/s]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  1.73it/s]\n",
      "\u001b[1;36m(EngineCore_DP0 pid=4125)\u001b[0;0m \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(EngineCore_DP0 pid=4125)\u001b[0;0m INFO 11-13 22:38:07 [gpu_model_runner.py:2653] Model loading took 0.7738 GiB and 26.713282 seconds\n",
      "\u001b[1;36m(EngineCore_DP0 pid=4125)\u001b[0;0m INFO 11-13 22:38:13 [backends.py:548] Using cache directory: /home/ell/.cache/vllm/torch_compile_cache/38b5f3c097/rank_0_0/backbone for vLLM's torch.compile\n",
      "\u001b[1;36m(EngineCore_DP0 pid=4125)\u001b[0;0m INFO 11-13 22:38:13 [backends.py:559] Dynamo bytecode transform time: 5.53 s\n",
      "\u001b[1;36m(EngineCore_DP0 pid=4125)\u001b[0;0m ERROR 11-13 22:38:14 [core.py:708] EngineCore failed to start.\n",
      "\u001b[1;36m(EngineCore_DP0 pid=4125)\u001b[0;0m ERROR 11-13 22:38:14 [core.py:708] Traceback (most recent call last):\n",
      "\u001b[1;36m(EngineCore_DP0 pid=4125)\u001b[0;0m ERROR 11-13 22:38:14 [core.py:708]   File \"/home/ell/playground/env/lib/python3.10/site-packages/vllm/v1/engine/core.py\", line 699, in run_engine_core\n",
      "\u001b[1;36m(EngineCore_DP0 pid=4125)\u001b[0;0m ERROR 11-13 22:38:14 [core.py:708]     engine_core = EngineCoreProc(*args, **kwargs)\n",
      "\u001b[1;36m(EngineCore_DP0 pid=4125)\u001b[0;0m ERROR 11-13 22:38:14 [core.py:708]   File \"/home/ell/playground/env/lib/python3.10/site-packages/vllm/v1/engine/core.py\", line 498, in __init__\n",
      "\u001b[1;36m(EngineCore_DP0 pid=4125)\u001b[0;0m ERROR 11-13 22:38:14 [core.py:708]     super().__init__(vllm_config, executor_class, log_stats,\n",
      "\u001b[1;36m(EngineCore_DP0 pid=4125)\u001b[0;0m ERROR 11-13 22:38:14 [core.py:708]   File \"/home/ell/playground/env/lib/python3.10/site-packages/vllm/v1/engine/core.py\", line 92, in __init__\n",
      "\u001b[1;36m(EngineCore_DP0 pid=4125)\u001b[0;0m ERROR 11-13 22:38:14 [core.py:708]     self._initialize_kv_caches(vllm_config)\n",
      "\u001b[1;36m(EngineCore_DP0 pid=4125)\u001b[0;0m ERROR 11-13 22:38:14 [core.py:708]   File \"/home/ell/playground/env/lib/python3.10/site-packages/vllm/v1/engine/core.py\", line 190, in _initialize_kv_caches\n",
      "\u001b[1;36m(EngineCore_DP0 pid=4125)\u001b[0;0m ERROR 11-13 22:38:14 [core.py:708]     self.model_executor.determine_available_memory())\n",
      "\u001b[1;36m(EngineCore_DP0 pid=4125)\u001b[0;0m ERROR 11-13 22:38:14 [core.py:708]   File \"/home/ell/playground/env/lib/python3.10/site-packages/vllm/v1/executor/abstract.py\", line 85, in determine_available_memory\n",
      "\u001b[1;36m(EngineCore_DP0 pid=4125)\u001b[0;0m ERROR 11-13 22:38:14 [core.py:708]     return self.collective_rpc(\"determine_available_memory\")\n",
      "\u001b[1;36m(EngineCore_DP0 pid=4125)\u001b[0;0m ERROR 11-13 22:38:14 [core.py:708]   File \"/home/ell/playground/env/lib/python3.10/site-packages/vllm/executor/uniproc_executor.py\", line 83, in collective_rpc\n",
      "\u001b[1;36m(EngineCore_DP0 pid=4125)\u001b[0;0m ERROR 11-13 22:38:14 [core.py:708]     return [run_method(self.driver_worker, method, args, kwargs)]\n",
      "\u001b[1;36m(EngineCore_DP0 pid=4125)\u001b[0;0m ERROR 11-13 22:38:14 [core.py:708]   File \"/home/ell/playground/env/lib/python3.10/site-packages/vllm/utils/__init__.py\", line 3122, in run_method\n",
      "\u001b[1;36m(EngineCore_DP0 pid=4125)\u001b[0;0m ERROR 11-13 22:38:14 [core.py:708]     return func(*args, **kwargs)\n",
      "\u001b[1;36m(EngineCore_DP0 pid=4125)\u001b[0;0m ERROR 11-13 22:38:14 [core.py:708]   File \"/home/ell/playground/env/lib/python3.10/site-packages/torch/utils/_contextlib.py\", line 120, in decorate_context\n",
      "\u001b[1;36m(EngineCore_DP0 pid=4125)\u001b[0;0m ERROR 11-13 22:38:14 [core.py:708]     return func(*args, **kwargs)\n",
      "\u001b[1;36m(EngineCore_DP0 pid=4125)\u001b[0;0m ERROR 11-13 22:38:14 [core.py:708]   File \"/home/ell/playground/env/lib/python3.10/site-packages/vllm/v1/worker/gpu_worker.py\", line 263, in determine_available_memory\n",
      "\u001b[1;36m(EngineCore_DP0 pid=4125)\u001b[0;0m ERROR 11-13 22:38:14 [core.py:708]     self.model_runner.profile_run()\n",
      "\u001b[1;36m(EngineCore_DP0 pid=4125)\u001b[0;0m ERROR 11-13 22:38:14 [core.py:708]   File \"/home/ell/playground/env/lib/python3.10/site-packages/vllm/v1/worker/gpu_model_runner.py\", line 3392, in profile_run\n",
      "\u001b[1;36m(EngineCore_DP0 pid=4125)\u001b[0;0m ERROR 11-13 22:38:14 [core.py:708]     = self._dummy_run(self.max_num_tokens, is_profile=True)\n",
      "\u001b[1;36m(EngineCore_DP0 pid=4125)\u001b[0;0m ERROR 11-13 22:38:14 [core.py:708]   File \"/home/ell/playground/env/lib/python3.10/site-packages/torch/utils/_contextlib.py\", line 120, in decorate_context\n",
      "\u001b[1;36m(EngineCore_DP0 pid=4125)\u001b[0;0m ERROR 11-13 22:38:14 [core.py:708]     return func(*args, **kwargs)\n",
      "\u001b[1;36m(EngineCore_DP0 pid=4125)\u001b[0;0m ERROR 11-13 22:38:14 [core.py:708]   File \"/home/ell/playground/env/lib/python3.10/site-packages/vllm/v1/worker/gpu_model_runner.py\", line 3152, in _dummy_run\n",
      "\u001b[1;36m(EngineCore_DP0 pid=4125)\u001b[0;0m ERROR 11-13 22:38:14 [core.py:708]     outputs = self.model(\n",
      "\u001b[1;36m(EngineCore_DP0 pid=4125)\u001b[0;0m ERROR 11-13 22:38:14 [core.py:708]   File \"/home/ell/playground/env/lib/python3.10/site-packages/vllm/compilation/cuda_graph.py\", line 121, in __call__\n",
      "\u001b[1;36m(EngineCore_DP0 pid=4125)\u001b[0;0m ERROR 11-13 22:38:14 [core.py:708]     return self.runnable(*args, **kwargs)\n",
      "\u001b[1;36m(EngineCore_DP0 pid=4125)\u001b[0;0m ERROR 11-13 22:38:14 [core.py:708]   File \"/home/ell/playground/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1773, in _wrapped_call_impl\n",
      "\u001b[1;36m(EngineCore_DP0 pid=4125)\u001b[0;0m ERROR 11-13 22:38:14 [core.py:708]     return self._call_impl(*args, **kwargs)\n",
      "\u001b[1;36m(EngineCore_DP0 pid=4125)\u001b[0;0m ERROR 11-13 22:38:14 [core.py:708]   File \"/home/ell/playground/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1784, in _call_impl\n",
      "\u001b[1;36m(EngineCore_DP0 pid=4125)\u001b[0;0m ERROR 11-13 22:38:14 [core.py:708]     return forward_call(*args, **kwargs)\n",
      "\u001b[1;36m(EngineCore_DP0 pid=4125)\u001b[0;0m ERROR 11-13 22:38:14 [core.py:708]   File \"/home/ell/playground/env/lib/python3.10/site-packages/vllm/model_executor/models/llama.py\", line 597, in forward\n",
      "\u001b[1;36m(EngineCore_DP0 pid=4125)\u001b[0;0m ERROR 11-13 22:38:14 [core.py:708]     model_output = self.model(input_ids, positions, intermediate_tensors,\n",
      "\u001b[1;36m(EngineCore_DP0 pid=4125)\u001b[0;0m ERROR 11-13 22:38:14 [core.py:708]   File \"/home/ell/playground/env/lib/python3.10/site-packages/vllm/compilation/decorators.py\", line 310, in __call__\n",
      "\u001b[1;36m(EngineCore_DP0 pid=4125)\u001b[0;0m ERROR 11-13 22:38:14 [core.py:708]     output = self.compiled_callable(*args, **kwargs)\n",
      "\u001b[1;36m(EngineCore_DP0 pid=4125)\u001b[0;0m ERROR 11-13 22:38:14 [core.py:708]   File \"/home/ell/playground/env/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py\", line 749, in compile_wrapper\n",
      "\u001b[1;36m(EngineCore_DP0 pid=4125)\u001b[0;0m ERROR 11-13 22:38:14 [core.py:708]     raise e.remove_dynamo_frames() from None  # see TORCHDYNAMO_VERBOSE=1\n",
      "\u001b[1;36m(EngineCore_DP0 pid=4125)\u001b[0;0m ERROR 11-13 22:38:14 [core.py:708]   File \"/home/ell/playground/env/lib/python3.10/site-packages/torch/_inductor/compile_fx.py\", line 923, in _compile_fx_inner\n",
      "\u001b[1;36m(EngineCore_DP0 pid=4125)\u001b[0;0m ERROR 11-13 22:38:14 [core.py:708]     raise InductorError(e, currentframe()).with_traceback(\n",
      "\u001b[1;36m(EngineCore_DP0 pid=4125)\u001b[0;0m ERROR 11-13 22:38:14 [core.py:708]   File \"/home/ell/playground/env/lib/python3.10/site-packages/torch/_inductor/compile_fx.py\", line 907, in _compile_fx_inner\n",
      "\u001b[1;36m(EngineCore_DP0 pid=4125)\u001b[0;0m ERROR 11-13 22:38:14 [core.py:708]     mb_compiled_graph = fx_codegen_and_compile(\n",
      "\u001b[1;36m(EngineCore_DP0 pid=4125)\u001b[0;0m ERROR 11-13 22:38:14 [core.py:708]   File \"/home/ell/playground/env/lib/python3.10/site-packages/torch/_inductor/compile_fx.py\", line 1578, in fx_codegen_and_compile\n",
      "\u001b[1;36m(EngineCore_DP0 pid=4125)\u001b[0;0m ERROR 11-13 22:38:14 [core.py:708]     return scheme.codegen_and_compile(gm, example_inputs, inputs_to_check, graph_kwargs)\n",
      "\u001b[1;36m(EngineCore_DP0 pid=4125)\u001b[0;0m ERROR 11-13 22:38:14 [core.py:708]   File \"/home/ell/playground/env/lib/python3.10/site-packages/torch/_inductor/compile_fx.py\", line 1168, in codegen_and_compile\n",
      "\u001b[1;36m(EngineCore_DP0 pid=4125)\u001b[0;0m ERROR 11-13 22:38:14 [core.py:708]     torch._dynamo.repro.after_aot.save_graph_repro(\n",
      "\u001b[1;36m(EngineCore_DP0 pid=4125)\u001b[0;0m ERROR 11-13 22:38:14 [core.py:708]   File \"/home/ell/playground/env/lib/python3.10/site-packages/torch/_dynamo/repro/after_aot.py\", line 364, in save_graph_repro\n",
      "\u001b[1;36m(EngineCore_DP0 pid=4125)\u001b[0;0m ERROR 11-13 22:38:14 [core.py:708]     generate_compiler_repro_string(\n",
      "\u001b[1;36m(EngineCore_DP0 pid=4125)\u001b[0;0m ERROR 11-13 22:38:14 [core.py:708]   File \"/home/ell/playground/env/lib/python3.10/site-packages/torch/_dynamo/repro/after_aot.py\", line 312, in generate_compiler_repro_string\n",
      "\u001b[1;36m(EngineCore_DP0 pid=4125)\u001b[0;0m ERROR 11-13 22:38:14 [core.py:708]     model_str += _cuda_system_info_comment()\n",
      "\u001b[1;36m(EngineCore_DP0 pid=4125)\u001b[0;0m ERROR 11-13 22:38:14 [core.py:708]   File \"/home/ell/playground/env/lib/python3.10/site-packages/torch/_dynamo/debug_utils.py\", line 257, in _cuda_system_info_comment\n",
      "\u001b[1;36m(EngineCore_DP0 pid=4125)\u001b[0;0m ERROR 11-13 22:38:14 [core.py:708]     cuda_version_out = subprocess.check_output([\"nvcc\", \"--version\"])\n",
      "\u001b[1;36m(EngineCore_DP0 pid=4125)\u001b[0;0m ERROR 11-13 22:38:14 [core.py:708]   File \"/usr/lib/python3.10/subprocess.py\", line 421, in check_output\n",
      "\u001b[1;36m(EngineCore_DP0 pid=4125)\u001b[0;0m ERROR 11-13 22:38:14 [core.py:708]     return run(*popenargs, stdout=PIPE, timeout=timeout, check=True,\n",
      "\u001b[1;36m(EngineCore_DP0 pid=4125)\u001b[0;0m ERROR 11-13 22:38:14 [core.py:708]   File \"/usr/lib/python3.10/subprocess.py\", line 503, in run\n",
      "\u001b[1;36m(EngineCore_DP0 pid=4125)\u001b[0;0m ERROR 11-13 22:38:14 [core.py:708]     with Popen(*popenargs, **kwargs) as process:\n",
      "\u001b[1;36m(EngineCore_DP0 pid=4125)\u001b[0;0m ERROR 11-13 22:38:14 [core.py:708]   File \"/usr/lib/python3.10/subprocess.py\", line 971, in __init__\n",
      "\u001b[1;36m(EngineCore_DP0 pid=4125)\u001b[0;0m ERROR 11-13 22:38:14 [core.py:708]     self._execute_child(args, executable, preexec_fn, close_fds,\n",
      "\u001b[1;36m(EngineCore_DP0 pid=4125)\u001b[0;0m ERROR 11-13 22:38:14 [core.py:708]   File \"/usr/lib/python3.10/subprocess.py\", line 1863, in _execute_child\n",
      "\u001b[1;36m(EngineCore_DP0 pid=4125)\u001b[0;0m ERROR 11-13 22:38:14 [core.py:708]     raise child_exception_type(errno_num, err_msg, err_filename)\n",
      "\u001b[1;36m(EngineCore_DP0 pid=4125)\u001b[0;0m ERROR 11-13 22:38:14 [core.py:708] torch._inductor.exc.InductorError: PermissionError: [Errno 13] Permission denied: 'nvcc'\n",
      "\u001b[1;36m(EngineCore_DP0 pid=4125)\u001b[0;0m ERROR 11-13 22:38:14 [core.py:708] \n",
      "\u001b[1;36m(EngineCore_DP0 pid=4125)\u001b[0;0m ERROR 11-13 22:38:14 [core.py:708] Set TORCHDYNAMO_VERBOSE=1 for the internal stack trace (please do this especially if you're reporting a bug to PyTorch). For even more developer context, set TORCH_LOGS=\"+dynamo\"\n",
      "\u001b[1;36m(EngineCore_DP0 pid=4125)\u001b[0;0m ERROR 11-13 22:38:14 [core.py:708] \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(EngineCore_DP0 pid=4125)\u001b[0;0m Process EngineCore_DP0:\n",
      "\u001b[1;36m(EngineCore_DP0 pid=4125)\u001b[0;0m Traceback (most recent call last):\n",
      "\u001b[1;36m(EngineCore_DP0 pid=4125)\u001b[0;0m   File \"/usr/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "\u001b[1;36m(EngineCore_DP0 pid=4125)\u001b[0;0m     self.run()\n",
      "\u001b[1;36m(EngineCore_DP0 pid=4125)\u001b[0;0m   File \"/usr/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "\u001b[1;36m(EngineCore_DP0 pid=4125)\u001b[0;0m     self._target(*self._args, **self._kwargs)\n",
      "\u001b[1;36m(EngineCore_DP0 pid=4125)\u001b[0;0m   File \"/home/ell/playground/env/lib/python3.10/site-packages/vllm/v1/engine/core.py\", line 712, in run_engine_core\n",
      "\u001b[1;36m(EngineCore_DP0 pid=4125)\u001b[0;0m     raise e\n",
      "\u001b[1;36m(EngineCore_DP0 pid=4125)\u001b[0;0m   File \"/home/ell/playground/env/lib/python3.10/site-packages/vllm/v1/engine/core.py\", line 699, in run_engine_core\n",
      "\u001b[1;36m(EngineCore_DP0 pid=4125)\u001b[0;0m     engine_core = EngineCoreProc(*args, **kwargs)\n",
      "\u001b[1;36m(EngineCore_DP0 pid=4125)\u001b[0;0m   File \"/home/ell/playground/env/lib/python3.10/site-packages/vllm/v1/engine/core.py\", line 498, in __init__\n",
      "\u001b[1;36m(EngineCore_DP0 pid=4125)\u001b[0;0m     super().__init__(vllm_config, executor_class, log_stats,\n",
      "\u001b[1;36m(EngineCore_DP0 pid=4125)\u001b[0;0m   File \"/home/ell/playground/env/lib/python3.10/site-packages/vllm/v1/engine/core.py\", line 92, in __init__\n",
      "\u001b[1;36m(EngineCore_DP0 pid=4125)\u001b[0;0m     self._initialize_kv_caches(vllm_config)\n",
      "\u001b[1;36m(EngineCore_DP0 pid=4125)\u001b[0;0m   File \"/home/ell/playground/env/lib/python3.10/site-packages/vllm/v1/engine/core.py\", line 190, in _initialize_kv_caches\n",
      "\u001b[1;36m(EngineCore_DP0 pid=4125)\u001b[0;0m     self.model_executor.determine_available_memory())\n",
      "\u001b[1;36m(EngineCore_DP0 pid=4125)\u001b[0;0m   File \"/home/ell/playground/env/lib/python3.10/site-packages/vllm/v1/executor/abstract.py\", line 85, in determine_available_memory\n",
      "\u001b[1;36m(EngineCore_DP0 pid=4125)\u001b[0;0m     return self.collective_rpc(\"determine_available_memory\")\n",
      "\u001b[1;36m(EngineCore_DP0 pid=4125)\u001b[0;0m   File \"/home/ell/playground/env/lib/python3.10/site-packages/vllm/executor/uniproc_executor.py\", line 83, in collective_rpc\n",
      "\u001b[1;36m(EngineCore_DP0 pid=4125)\u001b[0;0m     return [run_method(self.driver_worker, method, args, kwargs)]\n",
      "\u001b[1;36m(EngineCore_DP0 pid=4125)\u001b[0;0m   File \"/home/ell/playground/env/lib/python3.10/site-packages/vllm/utils/__init__.py\", line 3122, in run_method\n",
      "\u001b[1;36m(EngineCore_DP0 pid=4125)\u001b[0;0m     return func(*args, **kwargs)\n",
      "\u001b[1;36m(EngineCore_DP0 pid=4125)\u001b[0;0m   File \"/home/ell/playground/env/lib/python3.10/site-packages/torch/utils/_contextlib.py\", line 120, in decorate_context\n",
      "\u001b[1;36m(EngineCore_DP0 pid=4125)\u001b[0;0m     return func(*args, **kwargs)\n",
      "\u001b[1;36m(EngineCore_DP0 pid=4125)\u001b[0;0m   File \"/home/ell/playground/env/lib/python3.10/site-packages/vllm/v1/worker/gpu_worker.py\", line 263, in determine_available_memory\n",
      "\u001b[1;36m(EngineCore_DP0 pid=4125)\u001b[0;0m     self.model_runner.profile_run()\n",
      "\u001b[1;36m(EngineCore_DP0 pid=4125)\u001b[0;0m   File \"/home/ell/playground/env/lib/python3.10/site-packages/vllm/v1/worker/gpu_model_runner.py\", line 3392, in profile_run\n",
      "\u001b[1;36m(EngineCore_DP0 pid=4125)\u001b[0;0m     = self._dummy_run(self.max_num_tokens, is_profile=True)\n",
      "\u001b[1;36m(EngineCore_DP0 pid=4125)\u001b[0;0m   File \"/home/ell/playground/env/lib/python3.10/site-packages/torch/utils/_contextlib.py\", line 120, in decorate_context\n",
      "\u001b[1;36m(EngineCore_DP0 pid=4125)\u001b[0;0m     return func(*args, **kwargs)\n",
      "\u001b[1;36m(EngineCore_DP0 pid=4125)\u001b[0;0m   File \"/home/ell/playground/env/lib/python3.10/site-packages/vllm/v1/worker/gpu_model_runner.py\", line 3152, in _dummy_run\n",
      "\u001b[1;36m(EngineCore_DP0 pid=4125)\u001b[0;0m     outputs = self.model(\n",
      "\u001b[1;36m(EngineCore_DP0 pid=4125)\u001b[0;0m   File \"/home/ell/playground/env/lib/python3.10/site-packages/vllm/compilation/cuda_graph.py\", line 121, in __call__\n",
      "\u001b[1;36m(EngineCore_DP0 pid=4125)\u001b[0;0m     return self.runnable(*args, **kwargs)\n",
      "\u001b[1;36m(EngineCore_DP0 pid=4125)\u001b[0;0m   File \"/home/ell/playground/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1773, in _wrapped_call_impl\n",
      "\u001b[1;36m(EngineCore_DP0 pid=4125)\u001b[0;0m     return self._call_impl(*args, **kwargs)\n",
      "\u001b[1;36m(EngineCore_DP0 pid=4125)\u001b[0;0m   File \"/home/ell/playground/env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1784, in _call_impl\n",
      "\u001b[1;36m(EngineCore_DP0 pid=4125)\u001b[0;0m     return forward_call(*args, **kwargs)\n",
      "\u001b[1;36m(EngineCore_DP0 pid=4125)\u001b[0;0m   File \"/home/ell/playground/env/lib/python3.10/site-packages/vllm/model_executor/models/llama.py\", line 597, in forward\n",
      "\u001b[1;36m(EngineCore_DP0 pid=4125)\u001b[0;0m     model_output = self.model(input_ids, positions, intermediate_tensors,\n",
      "\u001b[1;36m(EngineCore_DP0 pid=4125)\u001b[0;0m   File \"/home/ell/playground/env/lib/python3.10/site-packages/vllm/compilation/decorators.py\", line 310, in __call__\n",
      "\u001b[1;36m(EngineCore_DP0 pid=4125)\u001b[0;0m     output = self.compiled_callable(*args, **kwargs)\n",
      "\u001b[1;36m(EngineCore_DP0 pid=4125)\u001b[0;0m   File \"/home/ell/playground/env/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py\", line 749, in compile_wrapper\n",
      "\u001b[1;36m(EngineCore_DP0 pid=4125)\u001b[0;0m     raise e.remove_dynamo_frames() from None  # see TORCHDYNAMO_VERBOSE=1\n",
      "\u001b[1;36m(EngineCore_DP0 pid=4125)\u001b[0;0m   File \"/home/ell/playground/env/lib/python3.10/site-packages/torch/_inductor/compile_fx.py\", line 923, in _compile_fx_inner\n",
      "\u001b[1;36m(EngineCore_DP0 pid=4125)\u001b[0;0m     raise InductorError(e, currentframe()).with_traceback(\n",
      "\u001b[1;36m(EngineCore_DP0 pid=4125)\u001b[0;0m   File \"/home/ell/playground/env/lib/python3.10/site-packages/torch/_inductor/compile_fx.py\", line 907, in _compile_fx_inner\n",
      "\u001b[1;36m(EngineCore_DP0 pid=4125)\u001b[0;0m     mb_compiled_graph = fx_codegen_and_compile(\n",
      "\u001b[1;36m(EngineCore_DP0 pid=4125)\u001b[0;0m   File \"/home/ell/playground/env/lib/python3.10/site-packages/torch/_inductor/compile_fx.py\", line 1578, in fx_codegen_and_compile\n",
      "\u001b[1;36m(EngineCore_DP0 pid=4125)\u001b[0;0m     return scheme.codegen_and_compile(gm, example_inputs, inputs_to_check, graph_kwargs)\n",
      "\u001b[1;36m(EngineCore_DP0 pid=4125)\u001b[0;0m   File \"/home/ell/playground/env/lib/python3.10/site-packages/torch/_inductor/compile_fx.py\", line 1168, in codegen_and_compile\n",
      "\u001b[1;36m(EngineCore_DP0 pid=4125)\u001b[0;0m     torch._dynamo.repro.after_aot.save_graph_repro(\n",
      "\u001b[1;36m(EngineCore_DP0 pid=4125)\u001b[0;0m   File \"/home/ell/playground/env/lib/python3.10/site-packages/torch/_dynamo/repro/after_aot.py\", line 364, in save_graph_repro\n",
      "\u001b[1;36m(EngineCore_DP0 pid=4125)\u001b[0;0m     generate_compiler_repro_string(\n",
      "\u001b[1;36m(EngineCore_DP0 pid=4125)\u001b[0;0m   File \"/home/ell/playground/env/lib/python3.10/site-packages/torch/_dynamo/repro/after_aot.py\", line 312, in generate_compiler_repro_string\n",
      "\u001b[1;36m(EngineCore_DP0 pid=4125)\u001b[0;0m     model_str += _cuda_system_info_comment()\n",
      "\u001b[1;36m(EngineCore_DP0 pid=4125)\u001b[0;0m   File \"/home/ell/playground/env/lib/python3.10/site-packages/torch/_dynamo/debug_utils.py\", line 257, in _cuda_system_info_comment\n",
      "\u001b[1;36m(EngineCore_DP0 pid=4125)\u001b[0;0m     cuda_version_out = subprocess.check_output([\"nvcc\", \"--version\"])\n",
      "\u001b[1;36m(EngineCore_DP0 pid=4125)\u001b[0;0m   File \"/usr/lib/python3.10/subprocess.py\", line 421, in check_output\n",
      "\u001b[1;36m(EngineCore_DP0 pid=4125)\u001b[0;0m     return run(*popenargs, stdout=PIPE, timeout=timeout, check=True,\n",
      "\u001b[1;36m(EngineCore_DP0 pid=4125)\u001b[0;0m   File \"/usr/lib/python3.10/subprocess.py\", line 503, in run\n",
      "\u001b[1;36m(EngineCore_DP0 pid=4125)\u001b[0;0m     with Popen(*popenargs, **kwargs) as process:\n",
      "\u001b[1;36m(EngineCore_DP0 pid=4125)\u001b[0;0m   File \"/usr/lib/python3.10/subprocess.py\", line 971, in __init__\n",
      "\u001b[1;36m(EngineCore_DP0 pid=4125)\u001b[0;0m     self._execute_child(args, executable, preexec_fn, close_fds,\n",
      "\u001b[1;36m(EngineCore_DP0 pid=4125)\u001b[0;0m   File \"/usr/lib/python3.10/subprocess.py\", line 1863, in _execute_child\n",
      "\u001b[1;36m(EngineCore_DP0 pid=4125)\u001b[0;0m     raise child_exception_type(errno_num, err_msg, err_filename)\n",
      "\u001b[1;36m(EngineCore_DP0 pid=4125)\u001b[0;0m torch._inductor.exc.InductorError: PermissionError: [Errno 13] Permission denied: 'nvcc'\n",
      "\u001b[1;36m(EngineCore_DP0 pid=4125)\u001b[0;0m \n",
      "\u001b[1;36m(EngineCore_DP0 pid=4125)\u001b[0;0m Set TORCHDYNAMO_VERBOSE=1 for the internal stack trace (please do this especially if you're reporting a bug to PyTorch). For even more developer context, set TORCH_LOGS=\"+dynamo\"\n",
      "\u001b[1;36m(EngineCore_DP0 pid=4125)\u001b[0;0m \n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Engine core initialization failed. See root cause above. Failed core proc(s): {}",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m model_id \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124munsloth/tinyllama-bnb-4bit\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m----> 2\u001b[0m llm \u001b[38;5;241m=\u001b[39m \u001b[43mLLM\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbfloat16\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mquantization\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mbitsandbytes\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mload_format\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mbitsandbytes\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgpu_memory_utilization\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m.5\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/playground/env/lib/python3.10/site-packages/vllm/entrypoints/llm.py:297\u001b[0m, in \u001b[0;36mLLM.__init__\u001b[0;34m(self, model, runner, convert, tokenizer, tokenizer_mode, skip_tokenizer_init, trust_remote_code, allowed_local_media_path, allowed_media_domains, tensor_parallel_size, dtype, quantization, revision, tokenizer_revision, seed, gpu_memory_utilization, swap_space, cpu_offload_gb, enforce_eager, disable_custom_all_reduce, hf_token, hf_overrides, mm_processor_kwargs, pooler_config, override_pooler_config, structured_outputs_config, kv_cache_memory_bytes, compilation_config, logits_processors, **kwargs)\u001b[0m\n\u001b[1;32m    294\u001b[0m log_non_default_args(engine_args)\n\u001b[1;32m    296\u001b[0m \u001b[38;5;66;03m# Create the Engine (autoselects V0 vs V1)\u001b[39;00m\n\u001b[0;32m--> 297\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mllm_engine \u001b[38;5;241m=\u001b[39m \u001b[43mLLMEngine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_engine_args\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    298\u001b[0m \u001b[43m    \u001b[49m\u001b[43mengine_args\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mengine_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43musage_context\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mUsageContext\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mLLM_CLASS\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    299\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mengine_class \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mllm_engine)\n\u001b[1;32m    301\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrequest_counter \u001b[38;5;241m=\u001b[39m Counter()\n",
      "File \u001b[0;32m~/playground/env/lib/python3.10/site-packages/vllm/v1/engine/llm_engine.py:177\u001b[0m, in \u001b[0;36mLLMEngine.from_engine_args\u001b[0;34m(cls, engine_args, usage_context, stat_loggers, enable_multiprocessing)\u001b[0m\n\u001b[1;32m    174\u001b[0m     enable_multiprocessing \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    176\u001b[0m \u001b[38;5;66;03m# Create the LLMEngine.\u001b[39;00m\n\u001b[0;32m--> 177\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mvllm_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvllm_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    178\u001b[0m \u001b[43m           \u001b[49m\u001b[43mexecutor_class\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexecutor_class\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    179\u001b[0m \u001b[43m           \u001b[49m\u001b[43mlog_stats\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mengine_args\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdisable_log_stats\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    180\u001b[0m \u001b[43m           \u001b[49m\u001b[43musage_context\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43musage_context\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    181\u001b[0m \u001b[43m           \u001b[49m\u001b[43mstat_loggers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstat_loggers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    182\u001b[0m \u001b[43m           \u001b[49m\u001b[43mmultiprocess_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43menable_multiprocessing\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/playground/env/lib/python3.10/site-packages/vllm/v1/engine/llm_engine.py:114\u001b[0m, in \u001b[0;36mLLMEngine.__init__\u001b[0;34m(self, vllm_config, executor_class, log_stats, usage_context, stat_loggers, mm_registry, use_cached_outputs, multiprocess_mode)\u001b[0m\n\u001b[1;32m    111\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput_processor\u001b[38;5;241m.\u001b[39mtracer \u001b[38;5;241m=\u001b[39m tracer\n\u001b[1;32m    113\u001b[0m \u001b[38;5;66;03m# EngineCore (gets EngineCoreRequests and gives EngineCoreOutputs)\u001b[39;00m\n\u001b[0;32m--> 114\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mengine_core \u001b[38;5;241m=\u001b[39m \u001b[43mEngineCoreClient\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmake_client\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    115\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmultiprocess_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmultiprocess_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    116\u001b[0m \u001b[43m    \u001b[49m\u001b[43masyncio_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    117\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvllm_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvllm_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    118\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexecutor_class\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexecutor_class\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    119\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlog_stats\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlog_stats\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    120\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    122\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlogger_manager: Optional[StatLoggerManager] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    123\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlog_stats:\n",
      "File \u001b[0;32m~/playground/env/lib/python3.10/site-packages/vllm/v1/engine/core_client.py:80\u001b[0m, in \u001b[0;36mEngineCoreClient.make_client\u001b[0;34m(multiprocess_mode, asyncio_mode, vllm_config, executor_class, log_stats)\u001b[0m\n\u001b[1;32m     76\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m EngineCoreClient\u001b[38;5;241m.\u001b[39mmake_async_mp_client(\n\u001b[1;32m     77\u001b[0m         vllm_config, executor_class, log_stats)\n\u001b[1;32m     79\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m multiprocess_mode \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m asyncio_mode:\n\u001b[0;32m---> 80\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mSyncMPClient\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvllm_config\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexecutor_class\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlog_stats\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     82\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m InprocClient(vllm_config, executor_class, log_stats)\n",
      "File \u001b[0;32m~/playground/env/lib/python3.10/site-packages/vllm/v1/engine/core_client.py:602\u001b[0m, in \u001b[0;36mSyncMPClient.__init__\u001b[0;34m(self, vllm_config, executor_class, log_stats)\u001b[0m\n\u001b[1;32m    600\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, vllm_config: VllmConfig, executor_class: \u001b[38;5;28mtype\u001b[39m[Executor],\n\u001b[1;32m    601\u001b[0m              log_stats: \u001b[38;5;28mbool\u001b[39m):\n\u001b[0;32m--> 602\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m    603\u001b[0m \u001b[43m        \u001b[49m\u001b[43masyncio_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    604\u001b[0m \u001b[43m        \u001b[49m\u001b[43mvllm_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvllm_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    605\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexecutor_class\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexecutor_class\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    606\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlog_stats\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlog_stats\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    607\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    609\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_dp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvllm_config\u001b[38;5;241m.\u001b[39mparallel_config\u001b[38;5;241m.\u001b[39mdata_parallel_size \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    610\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutputs_queue \u001b[38;5;241m=\u001b[39m queue\u001b[38;5;241m.\u001b[39mQueue[Union[EngineCoreOutputs, \u001b[38;5;167;01mException\u001b[39;00m]]()\n",
      "File \u001b[0;32m~/playground/env/lib/python3.10/site-packages/vllm/v1/engine/core_client.py:448\u001b[0m, in \u001b[0;36mMPClient.__init__\u001b[0;34m(self, asyncio_mode, vllm_config, executor_class, log_stats, client_addresses)\u001b[0m\n\u001b[1;32m    444\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstats_update_address \u001b[38;5;241m=\u001b[39m client_addresses\u001b[38;5;241m.\u001b[39mget(\n\u001b[1;32m    445\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstats_update_address\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    446\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    447\u001b[0m     \u001b[38;5;66;03m# Engines are managed by this client.\u001b[39;00m\n\u001b[0;32m--> 448\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m launch_core_engines(vllm_config, executor_class,\n\u001b[1;32m    449\u001b[0m                              log_stats) \u001b[38;5;28;01mas\u001b[39;00m (engine_manager,\n\u001b[1;32m    450\u001b[0m                                             coordinator,\n\u001b[1;32m    451\u001b[0m                                             addresses):\n\u001b[1;32m    452\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mresources\u001b[38;5;241m.\u001b[39mcoordinator \u001b[38;5;241m=\u001b[39m coordinator\n\u001b[1;32m    453\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mresources\u001b[38;5;241m.\u001b[39mengine_manager \u001b[38;5;241m=\u001b[39m engine_manager\n",
      "File \u001b[0;32m/usr/lib/python3.10/contextlib.py:142\u001b[0m, in \u001b[0;36m_GeneratorContextManager.__exit__\u001b[0;34m(self, typ, value, traceback)\u001b[0m\n\u001b[1;32m    140\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m typ \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    141\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 142\u001b[0m         \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgen\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    143\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n\u001b[1;32m    144\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[0;32m~/playground/env/lib/python3.10/site-packages/vllm/v1/engine/utils.py:732\u001b[0m, in \u001b[0;36mlaunch_core_engines\u001b[0;34m(vllm_config, executor_class, log_stats, num_api_servers)\u001b[0m\n\u001b[1;32m    729\u001b[0m \u001b[38;5;28;01myield\u001b[39;00m local_engine_manager, coordinator, addresses\n\u001b[1;32m    731\u001b[0m \u001b[38;5;66;03m# Now wait for engines to start.\u001b[39;00m\n\u001b[0;32m--> 732\u001b[0m \u001b[43mwait_for_engine_startup\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    733\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhandshake_socket\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    734\u001b[0m \u001b[43m    \u001b[49m\u001b[43maddresses\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    735\u001b[0m \u001b[43m    \u001b[49m\u001b[43mengines_to_handshake\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    736\u001b[0m \u001b[43m    \u001b[49m\u001b[43mparallel_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    737\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvllm_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcache_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    738\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlocal_engine_manager\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    739\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcoordinator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mproc\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mcoordinator\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    740\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/playground/env/lib/python3.10/site-packages/vllm/v1/engine/utils.py:785\u001b[0m, in \u001b[0;36mwait_for_engine_startup\u001b[0;34m(handshake_socket, addresses, core_engines, parallel_config, cache_config, proc_manager, coord_process)\u001b[0m\n\u001b[1;32m    783\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m coord_process \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m coord_process\u001b[38;5;241m.\u001b[39mexitcode \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    784\u001b[0m         finished[coord_process\u001b[38;5;241m.\u001b[39mname] \u001b[38;5;241m=\u001b[39m coord_process\u001b[38;5;241m.\u001b[39mexitcode\n\u001b[0;32m--> 785\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEngine core initialization failed. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    786\u001b[0m                        \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSee root cause above. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    787\u001b[0m                        \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFailed core proc(s): \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfinished\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    789\u001b[0m \u001b[38;5;66;03m# Receive HELLO and READY messages from the input socket.\u001b[39;00m\n\u001b[1;32m    790\u001b[0m eng_identity, ready_msg_bytes \u001b[38;5;241m=\u001b[39m handshake_socket\u001b[38;5;241m.\u001b[39mrecv_multipart()\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Engine core initialization failed. See root cause above. Failed core proc(s): {}"
     ]
    }
   ],
   "source": [
    "model_id = \"unsloth/tinyllama-bnb-4bit\"\n",
    "llm = LLM(model=model_id, dtype=torch.bfloat16, quantization=\"bitsandbytes\", load_format=\"bitsandbytes\", gpu_memory_utilization=.5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
